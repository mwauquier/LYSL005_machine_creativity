{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8LbrstYpKsHX"
      ],
      "authorship_tag": "ABX9TyM4B/iS5atOAqm6zOpZh73O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mwauquier/LYSL005_machine_creativity/blob/main/LYSL005_2022_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Générer du texte avec des transformers\n",
        "\n",
        "Dans un premier temps, nous allons voir comment générer du texte avec des transformers prêts à l'emploi, sans réel intervention relative à l'entraînement des modèles."
      ],
      "metadata": {
        "id": "8Hfn34nyn4sB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-2\n",
        "\n",
        "La famille des GPT (qui compte désormais plusieurs descendants) est une famille de transformers génératifs pré-entraînés."
      ],
      "metadata": {
        "id": "8LbrstYpKsHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "XDLLAQdXIJLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On importe les ressources nécessaires\n",
        "from transformers import pipeline, set_seed "
      ],
      "metadata": {
        "id": "lOrZTVjmK4yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On définit les paramètres liés au modèle (modèle, tâche, reproductibilité)\n",
        "generator = pipeline('text-generation', model='gpt2') \n",
        "set_seed(42) "
      ],
      "metadata": {
        "id": "WN8qwnanyM4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On génère n séquences de j caractères maximum à partir d'un prompt donné\n",
        "n = 5\n",
        "j = 30\n",
        "prompt = \"Hello, I'm a language model\" # I'm a prompt and\n",
        "generator(prompt, max_length=j, num_return_sequences=n)"
      ],
      "metadata": {
        "id": "xYiUfHKnyV-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT\n",
        "\n"
      ],
      "metadata": {
        "id": "3hVbWyAImh8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous utiliserons un code adapté à partir du travail de Wang et Cho (2019), et qui exploite des modèles en anglais."
      ],
      "metadata": {
        "id": "oTxuMItNLB7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pytorch_pretrained_bert"
      ],
      "metadata": {
        "id": "U44vxhKHkQUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbPWAGH7JVPN"
      },
      "outputs": [],
      "source": [
        "# Charger les librairies\n",
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load English pre-trained model (weights)\n",
        "model_version = 'bert-base-uncased'\n",
        "model = BertForMaskedLM.from_pretrained(model_version)\n",
        "model.eval()\n",
        "cuda = torch.cuda.is_available()\n",
        "if cuda:\n",
        "    model = model.cuda()\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=model_version.endswith(\"uncased\"))\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]\n",
        "\n",
        "def untokenize_batch(batch):\n",
        "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
        "\n",
        "def detokenize(sent):\n",
        "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
        "    new_sent = []\n",
        "    for i, tok in enumerate(sent):\n",
        "        if tok.startswith(\"##\"):\n",
        "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
        "        else:\n",
        "            new_sent.append(tok)\n",
        "    return new_sent\n",
        "\n",
        "CLS = '[CLS]'\n",
        "SEP = '[SEP]'\n",
        "MASK = '[MASK]'\n",
        "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
        "sep_id = tokenizer.convert_tokens_to_ids([SEP])[0]\n",
        "cls_id = tokenizer.convert_tokens_to_ids([CLS])[0]"
      ],
      "metadata": {
        "id": "u9fkJ18ILKvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le système commence par masquer l'intégralité des mots de la phrase, et regénère arbitrairement un mot à la fois.\n",
        "\n",
        "Plus précisément :\n",
        "- Tous les mots sont masqués\n",
        "- On retire aléatoirement un des masques et on génère un output à partir de la distribution des probabilités du modèle\n",
        "- On répère cette opération\n",
        "- Le système s'arrête quand il a convergé, ou quand le procédé dure trop longtemps."
      ],
      "metadata": {
        "id": "hMekbG92lNX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False, return_list=True):\n",
        "    \"\"\" Generate a word from from out[gen_idx]\n",
        "    \n",
        "    args:\n",
        "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
        "        - gen_idx (int): location for which to generate for\n",
        "        - top_k (int): if >0, only sample from the top k most probable words\n",
        "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
        "    \"\"\"\n",
        "    logits = out[:, gen_idx]\n",
        "    if temperature is not None:\n",
        "        logits = logits / temperature\n",
        "    if top_k > 0:\n",
        "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
        "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
        "        idx = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1)\n",
        "    elif sample:\n",
        "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
        "        idx = dist.sample().squeeze(-1)\n",
        "    else:\n",
        "        idx = torch.argmax(logits, dim=-1)\n",
        "    return idx.tolist() if return_list else idx\n",
        "  \n",
        "def get_init_text(seed_text, max_len, batch_size = 1, rand_init=False):\n",
        "    \"\"\" Get initial sentence by padding seed_text with either masks or random words to max_len \"\"\"\n",
        "    batch = [seed_text + [MASK] * max_len + [SEP] for _ in range(batch_size)]\n",
        "    #if rand_init:\n",
        "    #    for ii in range(max_len):\n",
        "    #        init_idx[seed_len+ii] = np.random.randint(0, len(tokenizer.vocab))\n",
        "    \n",
        "    return tokenize_batch(batch)\n",
        "\n",
        "def printer(sent, should_detokenize=True):\n",
        "    if should_detokenize:\n",
        "        sent = detokenize(sent)[1:-1]\n",
        "    print(\" \".join(sent))"
      ],
      "metadata": {
        "id": "YJTGvFZULPAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce code propose trois méthodes de génération : We consider three \"modes\" of generating:\n",
        "- Génère un mot à la fois à une position aléatoire (*parallel sequential generation*)\n",
        "- Génère tous les mots à la fois (*parallel generation*)\n",
        "- Génère un mot à la fois, de gauche à droite (*sequential generation*)\n",
        "\n",
        "Ces trois méthodes sont définies dans la cellule ci-dessous. Le choix de la méthode de génération se fait dans une fonction ultérieure."
      ],
      "metadata": {
        "id": "WTL_w8NWLSU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generation modes as functions\n",
        "import math\n",
        "import time\n",
        "\n",
        "def parallel_sequential_generation(seed_text, batch_size=10, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
        "                                   cuda=False, print_every=10, verbose=True):\n",
        "    \"\"\" Generate for one random position at a timestep\n",
        "    \n",
        "    args:\n",
        "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
        "    \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "    \n",
        "    for ii in range(max_iter):\n",
        "        kk = np.random.randint(0, max_len)\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = mask_id\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp)\n",
        "        topk = top_k if (ii >= burnin) else 0\n",
        "        idxs = generate_step(out, gen_idx=seed_len+kk, top_k=topk, temperature=temperature, sample=(ii < burnin))\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = idxs[jj]\n",
        "            \n",
        "        if verbose and np.mod(ii+1, print_every) == 0:\n",
        "            for_print = tokenizer.convert_ids_to_tokens(batch[0])\n",
        "            for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
        "            print(\"iter\", ii+1, \" \".join(for_print))\n",
        "            \n",
        "    return untokenize_batch(batch)\n",
        "\n",
        "def parallel_generation(seed_text, batch_size=10, max_len=15, top_k=0, temperature=None, max_iter=300, sample=True, \n",
        "                        cuda=False, print_every=10, verbose=True):\n",
        "    \"\"\" Generate for all positions at each time step \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "    \n",
        "    for ii in range(max_iter):\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp)\n",
        "        for kk in range(max_len):\n",
        "            idxs = generate_step(out, gen_idx=seed_len+kk, top_k=top_k, temperature=temperature, sample=sample)\n",
        "            for jj in range(batch_size):\n",
        "                batch[jj][seed_len+kk] = idxs[jj]\n",
        "            \n",
        "        if verbose and np.mod(ii, print_every) == 0:\n",
        "            print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(batch[0])))\n",
        "    \n",
        "    return untokenize_batch(batch)\n",
        "            \n",
        "def sequential_generation(seed_text, batch_size=10, max_len=15, leed_out_len=15, \n",
        "                          top_k=0, temperature=None, sample=True, cuda=False):\n",
        "    \"\"\" Generate one word at a time, in L->R order \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "    \n",
        "    for ii in range(max_len):\n",
        "        inp = [sent[:seed_len+ii+leed_out_len]+[sep_id] for sent in batch]\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp)\n",
        "        idxs = generate_step(out, gen_idx=seed_len+ii, top_k=top_k, temperature=temperature, sample=sample)\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+ii] = idxs[jj]\n",
        "        \n",
        "    return untokenize_batch(batch)\n",
        "\n",
        "\n",
        "def generate(n_samples, seed_text=\"[CLS]\", batch_size=10, max_len=25, \n",
        "             generation_mode=\"parallel-sequential\",\n",
        "             sample=True, top_k=100, temperature=1.0, burnin=200, max_iter=500,\n",
        "             cuda=False, print_every=1):\n",
        "    # main generation function to call\n",
        "    sentences = []\n",
        "    n_batches = math.ceil(n_samples / batch_size)\n",
        "    start_time = time.time()\n",
        "    for batch_n in range(n_batches):\n",
        "        if generation_mode == \"parallel-sequential\":\n",
        "            batch = parallel_sequential_generation(seed_text, batch_size=batch_size, max_len=max_len, top_k=top_k,\n",
        "                                                   temperature=temperature, burnin=burnin, max_iter=max_iter, \n",
        "                                                   cuda=cuda, verbose=False)\n",
        "        elif generation_mode == \"sequential\":\n",
        "            batch = sequential_generation(seed_text, batch_size=batch_size, max_len=max_len, top_k=top_k, \n",
        "                                          temperature=temperature, leed_out_len=leed_out_len, sample=sample,\n",
        "                                          cuda=cuda)\n",
        "        elif generation_mode == \"parallel\":\n",
        "            batch = parallel_generation(seed_text, batch_size=batch_size,\n",
        "                                        max_len=max_len, top_k=top_k, temperature=temperature, \n",
        "                                        sample=sample, max_iter=max_iter, \n",
        "                                        cuda=cuda, verbose=False)\n",
        "        \n",
        "        if (batch_n + 1) % print_every == 0:\n",
        "            print(\"Finished batch %d in %.3fs\" % (batch_n + 1, time.time() - start_time))\n",
        "            start_time = time.time()\n",
        "        \n",
        "        sentences += batch\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "azaC9LW6LWdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avant de générer notre texte, il nous faut fixer divers paramètres :\n",
        "\n",
        "- n_sample : le nombre de \"phrases\" à générer.\n",
        "- max_len : la longueur de la séquence à générer\n",
        "- top_k : la taille de l'échantillon des mots les plus probables dans lequel le système va piocher\n",
        "- temperature : un paramètre de lissage de la distribution des mots suivants. Plus la valeur sera élevée, plus cette distribution sera uniforme (lissée). Plus elle sera faible, moins elle sera lissée.\n",
        "- burnin : ce paramètre ne vaut que pour les méthodes de génération non séquentielle. Elle implique la neutralisation du paramètre top_k, en faisant piocher le système dans l'ensemble de la distribution, et non plus parmi l'échantillon des k mots les plus probables.\n",
        "- max_iter : nombre d'itérations\n",
        "- seed_text : Premier élément à partir duquel générer du texte. Il s'agit du prompt. Il contient le préfix [CLS], auquel vous pouvez ajouter du texte."
      ],
      "metadata": {
        "id": "nLX7rKcvLXaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 5\n",
        "batch_size = 5\n",
        "max_len = 40\n",
        "top_k = 100\n",
        "temperature = 1.0\n",
        "generation_mode = \"parallel-sequential\"\n",
        "leed_out_len = 5 # max_len\n",
        "burnin = 250\n",
        "sample = True\n",
        "max_iter = 500\n",
        "prompt = \"[CLS]\""
      ],
      "metadata": {
        "id": "2fcxDTpwLWnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il vous reste alors à mobiliser la fonction. En l'occurrence, les énoncés générés sont enregistrés dans la liste `bert_sents`, que l'on peut ensuite aller parcourir."
      ],
      "metadata": {
        "id": "0B_EpUJ58e5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the prefix context\n",
        "seed_text = prompt.split()\n",
        "bert_sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
        "                      generation_mode=generation_mode,\n",
        "                      sample=sample, top_k=top_k, temperature=temperature, burnin=burnin, max_iter=max_iter,\n",
        "                      cuda=cuda)"
      ],
      "metadata": {
        "id": "v0iI5HAp68UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in bert_sents:\n",
        "    printer(sent, should_detokenize=True)"
      ],
      "metadata": {
        "id": "Vg0znkXWLeeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous pouvez si vous le souhaitez imprimer les énoncés générés directement dans un fichier et non dans le notebook."
      ],
      "metadata": {
        "id": "vYHOeoyD83G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_sents(out_file, sents, should_detokenize=False):\n",
        "    with open(out_file, \"w\") as out_fh:\n",
        "        for sent in sents:\n",
        "            sent = detokenize(sent[1:-1]) if should_detokenize else sent\n",
        "            out_fh.write(\"%s\\n\" % \" \".join(sent))\n",
        "\n",
        "for temp in [1.0]:\n",
        "    bert_sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len, generation_mode=generation_mode,\n",
        "                          sample=sample, top_k=top_k, temperature=temp, burnin=burnin, max_iter=max_iter)\n",
        "                          #, cuda=True)\n",
        "    out_file = \"/path/to/outfile/%s-len%d-burnin%d-topk%d-temp%.3f.txt\" % (model_version, max_len, burnin, top_k, temp)\n",
        "    write_sents(out_file, bert_sents, should_detokenize=True)"
      ],
      "metadata": {
        "id": "O02gQdwdLmWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si les modèles GPT2 sont entraînés sur des données issues du web sans contrainte de pays ou de langue (et donc contenant majoritairement de l'anglais, mais pas que), il est spécialisé sur l'anglais.\n",
        "\n",
        "Vous pouvez utiliser des modèles GPT2 spécialement développés pour le français en cherchant dans la liste des modèles accessible à la page https://huggingface.co/models?sort=downloads . Il vous suffit de chercher dans la barre de recherche \"GPT2 french\". Attention, le nom à intégrer en argument doit intégrer le nom du répertoire s'il est indiqué dans le \"nom\" du modèle.\n",
        "\n",
        "Voici par exemple des noms de modèles GPT2 pour le français : \"dbddv01/gpt2-french-small\", \"ClassCat/gpt2-base-french\".\n",
        "\n",
        "Ils s'utilisent sinon comme n'importe quel autre modèle"
      ],
      "metadata": {
        "id": "epg03QEan2LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On importe les ressources nécessaires\n",
        "from transformers import pipeline, set_seed "
      ],
      "metadata": {
        "id": "B1JTEOVFmlwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On définit les paramètres liés au modèle (modèle, tâche, reproductibilité)\n",
        "generator = pipeline('text-generation', model='ClassCat/gpt2-base-french')\n",
        "set_seed(42)  "
      ],
      "metadata": {
        "id": "63gINJ5wmstH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On génère n séquences de j caractères maximum à partir d'un prompt donné\n",
        "n = 5\n",
        "j = 30\n",
        "prompt = \"Bonjour, je suis un modèle de langue et\" # I'm a prompt and\n",
        "generator(prompt, max_length=j, num_return_sequences=n)"
      ],
      "metadata": {
        "id": "WkfrKCg-m2nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning pour la génération de texte\n",
        "\n",
        "Notez que les modèles français précédemment présentés n'ont pas été entraînés à partir de rien (*from scratch*). Il s'agit en réalité de modèles qui ont été finetunés. Plus précisément, ce sont des modèles qui ont été adaptés de modèles initiaux (majoritairement anglais) pour prendre en compte des données en français. L'idée est de réentraîner le modèle (ou du moins de poursuivre son entraînement) en lui soumettant des données spécialisées (ici en français)."
      ],
      "metadata": {
        "id": "cH2pw7X3zRIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons voir comment finetuner des modèles en utilisant le code ci-dessous (adapté du tutoriel https://www.kaggle.com/code/changyeop/how-to-fine-tune-gpt-2-for-beginners )"
      ],
      "metadata": {
        "id": "xZlv5vX_038k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# À lancer si vous n'avez pas préalablement initialisé l'environnement\n",
        "\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "X1n4OycqmebG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On importe les librairies nécessaires\n",
        "\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "AroLLBjx_WvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On définit les fonctions nécessaires (chargement des données, )\n",
        "\n",
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, \n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator\n",
        "\n",
        "\n",
        "def train(train_file_path,model_name,\n",
        "          output_dir,\n",
        "          overwrite_output_dir,\n",
        "          per_device_train_batch_size,\n",
        "          num_train_epochs,\n",
        "          save_steps):\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "  data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "      \n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "  model.save_pretrained(output_dir)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "      )\n",
        "\n",
        "  trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "  )\n",
        "      \n",
        "  trainer.train()\n",
        "  trainer.save_model()"
      ],
      "metadata": {
        "id": "TkowuBndBZr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nécessaire pour permettre au Notebook Colab d'accèder à votre Drive et donc à vos données\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BgrCgcYLCv6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On définit les paramètres (les données de réentraînement, l'endroit où seront stockés les fichiers relatifs à l'entraînement, les paramètres)\n",
        "train_file_path = \"/content/drive/MyDrive/machine_creativity/test_corpus_wiki_espaceSchegen_20230102.txt\"\n",
        "model_name = 'ClassCat/gpt2-base-french'\n",
        "output_dir = '/content/drive/MyDrive/machine_creativity/result_finetuning_test_20230102'\n",
        "overwrite_output_dir = False\n",
        "per_device_train_batch_size = 10\n",
        "num_train_epochs = 1.0\n",
        "save_steps = 1000"
      ],
      "metadata": {
        "id": "G4mItzdoBppF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ré-entraînement du modèle sur vos données\n",
        "train(\n",
        "    train_file_path=train_file_path,\n",
        "    model_name=model_name,\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=overwrite_output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    save_steps=save_steps\n",
        ")"
      ],
      "metadata": {
        "id": "oPGfxKMgCUIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'entraînement peut prendre du temps. À guise d'exemple, le finetuning du modèle \"ClassCat/gpt2-base-french\" à partir d'un article (Siward Barn), d'un poids de 20kB, avec les paramètres per_device_train_batch_size = 10,\n",
        "num_train_epochs = 1.0, et save_steps = 1000, prend 4min. Ce même finetuning mais sur les 13 articles de la catégorie \"Espace Schengen\" de Wikipedia (Catégorie:Espace Schengen), pour un poids de 306kB, prendre 26min.\n",
        "\n",
        "L'étape suivante est celle de la génération de texte à proprement parler. Cependant, on ne peut pas ici reprendre la fonction `generator()` précédemment utilisée puisqu'il nous faut ici redéfinir les modèles et tokeniser à utiliser. Nous devons donc redéfinir la fonction de génération de texte à l'aide du code ci-dessous."
      ],
      "metadata": {
        "id": "_lzGlv2apkF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast, GPT2TokenizerFast\n",
        "\n",
        "def load_model(model_path):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def generate_text(sequence, max_length):\n",
        "    model_path = \"/content/drive/MyDrive/machine_creativity/result_finetuning_test_20230102\"\n",
        "    model = load_model(model_path)\n",
        "    tokenizer = load_tokenizer(model_path)\n",
        "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
        "    final_outputs = model.generate(\n",
        "        ids,\n",
        "        do_sample=True,\n",
        "        max_length=max_length,\n",
        "        pad_token_id=model.config.eos_token_id,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "xr7zvogqnGLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il suffit alors d'utiliser la fonction pour générer du texte avec notre modèle finetuné."
      ],
      "metadata": {
        "id": "6X66bCa33V4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\n",
        "max_len = 100\n",
        "generate_text(prompt, max_len)"
      ],
      "metadata": {
        "id": "SvXxMtS3nTp-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}